# -*- coding: utf-8 -*-
"""Pyhton1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z8JCFt0qn61N_371TepBryXLcIlKVK-U
"""

# === Single Colab cell: Study Mate (Translation, PDF Q/A, PDF Summary) ===
# Copy & paste this whole cell into Google Colab and run.

# 0) (Optional) If you already set HF_TOKEN in a previous cell, ok.
# Example (run before this cell if needed):
# import os; os.environ["HF_TOKEN"] = "hf_xxx"

# 1) Install required packages (this will take ~1-3 minutes)
# - poppler-utils and tesseract (system) for OCR/pdf->image conversion
# - python packages for models, gradio, pdf parsing and OCR
!apt-get update -qq
!apt-get install -y -qq poppler-utils tesseract-ocr

# Python packages
!pip install --quiet "transformers>=4.49" accelerate huggingface_hub gradio pillow PyPDF2 pdf2image pytesseract
# Install CPU torch (change to GPU wheel manually if you prefer)
!pip install --quiet torch --index-url https://download.pytorch.org/whl/cpu

# 2) Imports
import os, io, math, textwrap
from typing import List
import torch
from transformers import pipeline, AutoProcessor, AutoModelForCausalLM, AutoModelForSeq2SeqLM
from huggingface_hub import login as hf_login
from PyPDF2 import PdfReader
from pdf2image import convert_from_bytes
import pytesseract
from PIL import Image
import gradio as gr

# 3) Configuration
TEXT_MODEL_ID = "ibm-granite/granite-3.2-2b-instruct"
# Optionally set HF_TOKEN env var prior to running this cell:
HF_TOKEN = os.environ.get("HF_TOKEN", None)
if HF_TOKEN:
    try:
        hf_login(HF_TOKEN)
        print("[INFO] Hugging Face token set via HF_TOKEN.")
    except Exception as e:
        print("[WARN] HF token login failed:", e)

DEVICE = 0 if torch.cuda.is_available() else -1
print(f"[INFO] Using device: {'cuda' if DEVICE==0 else 'cpu'}")

# 4) Lazy loading pipeline
_text_pipe = None
def load_text_pipe():
    global _text_pipe
    if _text_pipe is None:
        print(f"[INFO] Loading text model: {TEXT_MODEL_ID} (may download large weights)...")
        _text_pipe = pipeline(
            "text-generation",
            model=TEXT_MODEL_ID,
            device=DEVICE,
            trust_remote_code=True,
            max_new_tokens=512,
            do_sample=False
        )
        print("[INFO] Text model loaded.")
    return _text_pipe

def run_instruction(prompt: str, max_new_tokens:int=512) -> str:
    pipe = load_text_pipe()
    out = pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False)
    # pipeline usually returns list of dicts with 'generated_text'
    if isinstance(out, list) and out and "generated_text" in out[0]:
        return out[0]["generated_text"].strip()
    return str(out)

# 5) PDF text extraction (with OCR fallback)
def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    # Try text extraction first
    try:
        reader = PdfReader(io.BytesIO(pdf_bytes))
        pages_text = []
        for p in reader.pages:
            try:
                pages_text.append(p.extract_text() or "")
            except Exception:
                pages_text.append("")
        combined = "\n\n".join(pages_text).strip()
    except Exception as e:
        combined = ""
    # If no text found, try OCR via pdf2image + pytesseract
    if not combined or len(combined.strip()) < 50:
        try:
            images = convert_from_bytes(pdf_bytes, dpi=200)
            ocr_pages = []
            for img in images:
                txt = pytesseract.image_to_string(img)
                ocr_pages.append(txt)
            combined = "\n\n".join(ocr_pages).strip()
            if combined:
                combined = "[OCR extracted text]\n\n" + combined
        except Exception as e:
            # OCR failed or not available
            pass
    return combined

# 6) Text chunking helpers
def chunk_text(text: str, max_chars: int = 3000) -> List[str]:
    if not text:
        return []
    chunks = []
    start = 0
    L = len(text)
    while start < L:
        end = min(start + max_chars, L)
        cut = text.rfind("\n", start, end)
        if cut <= start:
            cut = text.rfind(".", start, end)
        if cut <= start:
            cut = end
        chunks.append(text[start:cut].strip())
        start = cut
    return chunks

def choose_best_chunk(chunks: List[str], question: str) -> str:
    if not chunks:
        return ""
    qset = set([w.lower().strip(".,?;:()[]\"'") for w in question.split()])
    best_idx, best_score = 0, -1
    for i, c in enumerate(chunks):
        cset = set([w.lower().strip(".,?;:()[]\"'") for w in c.split()])
        score = len(qset & cset)
        if score > best_score:
            best_idx, best_score = i, score
    return chunks[best_idx]

# 7) Feature: Translation
def translate_text(user_text: str, target_language: str = "English") -> str:
    if not user_text or not user_text.strip():
        return "Please provide text to translate."
    prompt = (
        f"You are a helpful translator. Translate the following text into {target_language}.\n\n"
        f"Text:\n\"\"\"\n{user_text}\n\"\"\"\n\nProvide only the translated text."
    )
    return run_instruction(prompt, max_new_tokens=512)

# 8) Feature: PDF Q/A
def pdf_qa(pdf_file, question: str):
    if pdf_file is None:
        return "Upload a PDF first."
    # Read bytes from Gradio file object (works in Colab)
    try:
        if isinstance(pdf_file, dict) and "name" in pdf_file:
            path = pdf_file["name"]
            with open(path, "rb") as f:
                b = f.read()
        elif hasattr(pdf_file, "name"):
            with open(pdf_file.name, "rb") as f:
                b = f.read()
        else:
            b = pdf_file.read()
    except Exception as e:
        return f"Failed to read uploaded PDF: {e}"
    if not question or not question.strip():
        return "Please enter a question about the PDF."
    pdf_text = extract_text_from_pdf_bytes(b)
    if not pdf_text:
        return "No extractable text found in PDF. If it's scanned, try enabling OCR or ensure Tesseract is installed."
    chunks = chunk_text(pdf_text, max_chars=3000)
    best = choose_best_chunk(chunks, question)
    if not best:
        return "Could not find a relevant excerpt in the PDF."
    prompt = (
        "You are a helpful study assistant. Use ONLY the information in the excerpt below to answer the question.\n\n"
        "Excerpt:\n\"\"\"\n" + best + "\n\"\"\"\n\n"
        "Question:\n" + question + "\n\nAnswer concisely. If the answer is not in the excerpt, say 'I cannot find the answer in the supplied material.'"
    )
    return run_instruction(prompt, max_new_tokens=512)

# 9) Feature: PDF Summary (hierarchical approach)
def summarize_pdf(pdf_file, length: str = "short"):
    if pdf_file is None:
        return "Upload a PDF first."
    try:
        if isinstance(pdf_file, dict) and "name" in pdf_file:
            path = pdf_file["name"]
            with open(path, "rb") as f:
                b = f.read()
        elif hasattr(pdf_file, "name"):
            with open(pdf_file.name, "rb") as f:
                b = f.read()
        else:
            b = pdf_file.read()
    except Exception as e:
        return f"Failed to read uploaded PDF: {e}"
    pdf_text = extract_text_from_pdf_bytes(b)
    if not pdf_text:
        return "No extractable text found in PDF. If scanned PDF, OCR is required and tesseract must be installed."
    summary_style = "short"
    if length and length.lower() in ("long","detailed"):
        summary_style = "long"
    # If small doc summarize directly
    if len(pdf_text) < 6000:
        prompt = (
            f"You are a helpful study assistant. Provide a {summary_style} summary of the study material below.\n\n"
            "Study material:\n\"\"\"\n" + pdf_text + "\n\"\"\"\n\nSummary:"
        )
        return run_instruction(prompt, max_new_tokens=512)
    # For long doc: chunk->summarize chunks->combine->finalize
    chunks = chunk_text(pdf_text, max_chars=2800)
    chunk_summaries = []
    for i, c in enumerate(chunks):
        prompt_chunk = (
            "You are a helpful study assistant. Summarize the following excerpt briefly (2-4 sentences):\n\n"
            "Excerpt:\n\"\"\"\n" + c + "\n\"\"\"\n\nSummary:"
        )
        s = run_instruction(prompt_chunk, max_new_tokens=256)
        chunk_summaries.append(s.strip())
    combined = "\n\n".join(chunk_summaries)
    final_prompt = (
        f"You are a helpful study assistant. Produce a {summary_style} coherent summary from the following chunk summaries:\n\n"
        "Chunk summaries:\n\"\"\"\n" + combined + "\n\"\"\"\n\nFinal summary:"
    )
    final_summary = run_instruction(final_prompt, max_new_tokens=512)
    return final_summary

# 10) Gradio UI
with gr.Blocks(title="Study Mate â€” Translation | PDF Q/A | PDF Summary (Granite 3.2-2b)") as demo:
    gr.Markdown("# Study Mate\nTranslate text, ask Q/A from PDFs, and summarize PDFs (IBM Granite 3.2-2b Instruct)")
    with gr.Tabs():
        with gr.TabItem("Translate"):
            txt_in = gr.Textbox(lines=6, label="Text to translate")
            tgt_lang = gr.Textbox(label="Target language (e.g., English, Hindi, French)", value="English")
            tr_btn = gr.Button("Translate")
            tr_out = gr.Textbox(lines=6, label="Translation")
            tr_btn.click(fn=translate_text, inputs=[txt_in, tgt_lang], outputs=[tr_out])

        with gr.TabItem("PDF Q/A"):
            pdf_file = gr.File(label="Upload PDF (text PDFs or scanned PDFs)")
            question = gr.Textbox(lines=2, label="Question about the PDF")
            qa_btn = gr.Button("Ask")
            qa_out = gr.Textbox(lines=6, label="Answer")
            qa_btn.click(fn=pdf_qa, inputs=[pdf_file, question], outputs=[qa_out])

        with gr.TabItem("PDF Summary"):
            pdf_file2 = gr.File(label="Upload PDF")
            length = gr.Radio(choices=["short","long"], value="short", label="Summary length")
            sum_btn = gr.Button("Summarize")
            sum_out = gr.Textbox(lines=8, label="Summary")
            sum_btn.click(fn=summarize_pdf, inputs=[pdf_file2, length], outputs=[sum_out])

    gr.Markdown("**Notes:** Models are large and downloads may take several minutes. If you see permission errors, set HF_TOKEN in Colab prior to running (see top notes). For faster runs, enable GPU in Runtime settings.")
# Launch
demo.launch(share=False, inbrowser=True)